{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from edward.models import Normal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "def gen_time_series(n_points):\n",
    "    time_series = np.array([[np.random.normal(mu*i, sigma*i), \n",
    "                        np.random.normal(mu*i, sigma*i/4)] \n",
    "                       for i in range(n_points)])\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GLOBAL_SEED=1\n",
    "TESTING=0\n",
    "\n",
    "mu=1\n",
    "sigma=1\n",
    "n_series=100\n",
    "n_points = 100\n",
    "n_features = 2\n",
    "# multi-variate time-series: \n",
    "# x_t ~ [Gaussian(mu*t,sigma*t), Gaussian(mu*t,sigma*t/4)]\n",
    "# Points X Time X Features\n",
    "x_data = np.zeros((n_series,n_points,n_features), dtype=np.float32)\n",
    "for i in range(n_series):\n",
    "    x_data[i] = gen_time_series(n_points)\n",
    "\n",
    "if TESTING: x_data_test=x_data[:10]\n",
    "plt.title(\"Sample bi-variate time series\")\n",
    "for i in range(n_features):\n",
    "    plt.plot(x_data[0,:,i]);\n",
    "print(x_data.shape)\n",
    "print(reduce(lambda x,y: x*y, list(x_data.shape)), \"data values\")\n",
    "\n",
    "x_train = x_data[:70]\n",
    "x_validate = x_data[70:90]\n",
    "x_test = x_data[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapper around tf.get_variable that checks if the variable has already been defined.\n",
    "# useful for playing around in jupyter. allows you to call cells multiple times\n",
    "def get_variable_wrap(*args, **kwargs):\n",
    "    try:\n",
    "        return tf.get_variable(*args, **kwargs)\n",
    "    except ValueError:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        return tf.get_variable(*args, **kwargs)\n",
    "\n",
    "def fc_act(x, next_layer_size, act=None, name=\"fc\"):\n",
    "    nbatches = x.get_shape()[0]\n",
    "    prev_layer_size = x.get_shape()[1]\n",
    "    with tf.name_scope(\"fc\"):\n",
    "        w = get_variable_wrap(\"weights\", [prev_layer_size, next_layer_size], dtype=tf.float32, initializer=tf.random_normal_initializer())\n",
    "        b = get_variable_wrap(\"bias\", [next_layer_size], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        o = tf.add(tf.matmul(x, w), b)\n",
    "        if act: return act(o)\n",
    "        else: return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_lstm_cell(rnn_size, batch_size):\n",
    "    with tf.variable_scope(\"lstm\"):\n",
    "        try:\n",
    "            cell = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True)\n",
    "        except:\n",
    "            cell = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True, reuse=True)\n",
    "        # get initial_state of rnn\n",
    "        state = cell.zero_state(batch_size, tf.float32)\n",
    "    return cell, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prior(rnn_hidden, prior_size, z_size):\n",
    "    with tf.variable_scope(\"prior\"):\n",
    "        with tf.variable_scope(\"prior\"):\n",
    "            prior = fc_act(rnn_hidden, prior_size, act=tf.nn.relu)\n",
    "        with tf.variable_scope(\"prior_mu\"):\n",
    "            prior_mu = fc_act(prior, z_size)\n",
    "        with tf.variable_scope(\"prior_sigma\"):\n",
    "            prior_sigma = fc_act(prior, z_size, act=tf.nn.softplus)\n",
    "    return [prior, \n",
    "            prior_mu, \n",
    "            prior_sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_decoding(rnn_hidden, theta_size, x_size):\n",
    "    with tf.variable_scope(\"decoder\"):\n",
    "        with tf.variable_scope(\"theta\"):\n",
    "            theta = fc_act(rnn_hidden, theta_size, act=tf.nn.relu)\n",
    "        with tf.variable_scope(\"theta_mu\"):\n",
    "            theta_mu = fc_act(theta, x_size)\n",
    "        with tf.variable_scope(\"theta_sigma\"):\n",
    "            theta_sigma = fc_act(theta, x_size, act=tf.nn.softplus)\n",
    "    return [theta,\n",
    "            theta_mu,\n",
    "            theta_sigma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent RNN Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vrnn_recurrence(rnn_cell, z_1, state):\n",
    "    try:\n",
    "        return rnn_cell(z_1, state, \"rnn\")\n",
    "    except:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        return rnn_cell(z_1, state, \"rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VRNN(object):\n",
    "    \"\"\"docstring for VRNN\"\"\"\n",
    "    def __init__(self, x_t, batch_size, rnn_size, x_size, x_1_size, \n",
    "               z_size, z_1_size, phi_size, prior_size, theta_size):\n",
    "        self.x_t = x_t # place holder for time-series\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.x_size = x_size\n",
    "        self.x_1_size = x_1_size\n",
    "        self.z_size = z_size\n",
    "        self.z_1_size = z_1_size\n",
    "        self.phi_size = phi_size\n",
    "        self.prior_size = prior_size\n",
    "        self.theta_size = theta_size\n",
    "        \n",
    "        self.cell, self.state = init_lstm_cell(rnn_size, batch_size)\n",
    "        print (\"cell\", self.cell)\n",
    "        print (\"state\", self.state)\n",
    "        self._build()\n",
    "\n",
    "    def reset_cell(self): \n",
    "        self.state = self.cell.zero_state(self.batch_size, tf.float32)\n",
    "    \n",
    "    def _vrnn_step(self, vals, x_t):\n",
    "        # get latest values\n",
    "        # I chose to store them all but you don't HAVE to do this. \n",
    "        # This is a convenient thing to do however so you can check values later on\n",
    "        cell_state = vals[0]\n",
    "\n",
    "        prev_hidden = vals[1]\n",
    "\n",
    "        # mu_phi, sigma_phi = f(ht-1)\n",
    "        prior_vals = [prior, \n",
    "                prior_mu, \n",
    "                prior_sigma] = \\\n",
    "                    get_prior(prev_hidden, self.prior_size, self.z_size)\n",
    "        \n",
    "        # sample from prior\n",
    "        epsilon = tf.random_normal(shape=[x_t.get_shape().as_list()[0], z_size], seed=GLOBAL_SEED)\n",
    "        # z = mu + epsilon*sigma\n",
    "        z_t = tf.add(prior_mu, tf.multiply(epsilon, prior_sigma))\n",
    "          \n",
    "        z_1 = fc_act(z_t, z_1_size, act=tf.nn.relu)\n",
    "            \n",
    "        cell_output, cell_state = vrnn_recurrence(self.cell, z_1, prev_hidden)\n",
    "\n",
    "        # mu_x , sigma_x = f(ht)\n",
    "        decoding_vals = [theta,\n",
    "                theta_mu,\n",
    "                theta_sigma] = \\\n",
    "                    get_decoding(prev_hidden,self.theta_size, self.x_size)\n",
    "\n",
    "        return (cell_state[0],\n",
    "                cell_state[1],\n",
    "                cell_output\n",
    "               )\n",
    "    \n",
    "    def _build(self):\n",
    "        self.output = self._scan_sequence(tf.transpose(self.x_t, [1, 0, 2]))\n",
    "        self.nll = negative_log_likelihood_gaussian(self.x_t, output[-2], output[-1])\n",
    "        self.klgg = kl_gaussian_gaussian(output[4], output[5], \n",
    "                                    output[7], output[8])\n",
    "        self.objective = tf.add(tf.reduce_mean(nll),tf.reduce_mean(klgg))\n",
    "        self.train_op = get_optimizer(self.objective)\n",
    "    \n",
    "    def _scan_sequence(self, sequence):\n",
    "        return tf.scan(self._vrnn_step, sequence, initializer=(\n",
    "                self.state[0], # cell_state - 0,\n",
    "                self.state[1] # hidden_state - 1,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting for VRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test computing latent variable\n",
    "batch_size=10\n",
    "rnn_size=20\n",
    "\n",
    "x_size=x_data.shape[1]\n",
    "x_1_size=x_data.shape[1]\n",
    "\n",
    "z_size=15\n",
    "z_1_size=15\n",
    "\n",
    "phi_size = 25   # N ~ q(z|x)\n",
    "prior_size = 25 # N ~ p(z)\n",
    "theta_size = 25 # N ~ p(x|z)\n",
    "\n",
    "#get shape\n",
    "x_t_shape = list(x_data.shape)\n",
    "nbatches=max(x_t_shape[0]//batch_size,1)\n",
    "x_t_shape[0] = int(max(x_t_shape[0]/nbatches,1))\n",
    "print(\"nbatches\", nbatches)\n",
    "print(\"input shape\", x_t_shape)\n",
    "# input\n",
    "x_t = tf.placeholder(tf.float32, shape=x_t_shape, name=\"x_t\")\n",
    "\n",
    "vrnn = VRNN(x_t, batch_size, rnn_size, x_size, x_1_size, \n",
    "       z_size, z_1_size, phi_size, prior_size, theta_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
