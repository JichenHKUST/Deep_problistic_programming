{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from functools import reduce\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "GLOBAL_SEED=1\n",
    "VERBOSITY=1\n",
    "TESTING=0\n",
    "\n",
    "np.random.seed(GLOBAL_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# useful function for recovering functions in deleted cells\n",
    "def rescue_code(function):\n",
    "    import inspect\n",
    "    get_ipython().set_next_input(\"\".join(inspect.getsourcelines(function)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapper around tf.get_variable that checks if the variable has already been defined.\n",
    "# useful for playing around in jupyter. allows you to call cells multiple times\n",
    "def get_variable_wrap(*args, **kwargs):\n",
    "    try:\n",
    "        return tf.get_variable(*args, **kwargs)\n",
    "    except ValueError:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        return tf.get_variable(*args, **kwargs)\n",
    "\n",
    "## do matrix multiplication with weights and add bias (as in fully connected layer)\n",
    "def fc_act(x, next_layer_size, act=None, name=\"fc\"):\n",
    "    nbatches = x.get_shape()[0]\n",
    "    prev_layer_size = x.get_shape()[1]\n",
    "    with tf.name_scope(\"fc\"):\n",
    "        w = get_variable_wrap(\"weights\", [prev_layer_size, next_layer_size], dtype=tf.float32, initializer=tf.random_normal_initializer())\n",
    "        b = get_variable_wrap(\"bias\", [next_layer_size], dtype=tf.float32, initializer=tf.constant_initializer(0.1))\n",
    "        o = tf.add(tf.matmul(x, w), b)\n",
    "        if act: return act(o)\n",
    "        else: return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Data\n",
    "I want to create 100 bi-variate time-series sampled from the same distribution and split them into train/validation/test 70/20/10, i.e. split into groups of time-series size 14/4/2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_time_series(n_points):\n",
    "    time_series = np.array([[np.random.normal(mu*i, sigma*i), \n",
    "                        np.random.normal(mu*i, sigma*i/4)] \n",
    "                       for i in range(n_points)])\n",
    "    return time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mu=1\n",
    "sigma=1\n",
    "n_series=100\n",
    "n_points = 100\n",
    "n_features = 2\n",
    "# multi-variate time-series: \n",
    "# x_t ~ [Gaussian(mu*t,sigma*t), Gaussian(mu*t,sigma*t/4)]\n",
    "# Points X Time X Features\n",
    "x_data = np.zeros((n_series,n_points,n_features), dtype=np.float32)\n",
    "for i in range(n_series):\n",
    "    x_data[i] = gen_time_series(n_points)\n",
    "\n",
    "if TESTING: x_data_test=x_data[:10]\n",
    "plt.title(\"Sample bi-variate time series\")\n",
    "for i in range(n_features):\n",
    "    plt.plot(x_data[0,:,i]);\n",
    "print(x_data.shape)\n",
    "print(reduce(lambda x,y: x*y, list(x_data.shape)), \"data values\")\n",
    "\n",
    "x_train = x_data[:70]\n",
    "x_validate = x_data[70:90]\n",
    "x_test = x_data[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Variational RNN\n",
    "\n",
    "note that all gaussian distribtutions: p(z), q(z|x), p(x|z) should have the same dimensionality, i.e. share z_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First I'm going to define the loss functions necessary because they don't depend on anything\n",
    "def negative_log_likelihood_gaussian(y, mu, sigma):\n",
    "    \"\"\"\n",
    "                a                  b              c\n",
    "    (1/2)*[ ((y-mu)/sigma)^2 + 2*log(sigma) + log(2*pi) ]\n",
    "    learn more: https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood\n",
    "    \"\"\"\n",
    "#     sigma might be really small. in that case, a & b blow up\n",
    "    sigma=tf.clip_by_value(sigma, 1e-3,1e10)\n",
    "    a = tf.square(tf.div(tf.subtract(y,mu), sigma))\n",
    "    b = 2*tf.log(sigma)\n",
    "    c = tf.log(2*np.pi)\n",
    "    return tf.add(tf.add(a,b),c)\n",
    "\n",
    "def kl_gaussian_gaussian(mu1, sigma1, mu2, sigma2):\n",
    "    \"\"\"\n",
    "            a                  b         c           d\n",
    "    log(sigma2/sigma1) + (sigma1^2 + (mu1-mu2)^2)/2sigma2^2 - 1/2\n",
    "    \"\"\"\n",
    "    # Note: if sigma1 or sigma2 go to 0, this explodes\n",
    "    sigma1=tf.clip_by_value(sigma1, 1e-3,1e10)\n",
    "    sigma2=tf.clip_by_value(sigma2,1e-3,1e10)\n",
    "    a = tf.log(tf.divide(sigma2,sigma1))\n",
    "    b = tf.square(sigma1)\n",
    "    c = tf.square(tf.subtract(mu1,mu2))\n",
    "    d = 2*tf.square(sigma2)\n",
    "    e = tf.divide(tf.add(b,c), d)\n",
    "    return tf.add(a,e) - 0.5\n",
    "\n",
    "# I will also define a simple wrapper around reshape to flatten the 1st two dimensions of a tensor\n",
    "def flatten(x):\n",
    "    return tf.reshape(x, [int(x.shape[0])*int(x.shape[1]), -1])\n",
    "\n",
    "def tf_sample(batch_size, layer_size, mu, sigma):\n",
    "    epsilon = tf.random_normal(shape=[batch_size, layer_size], seed=GLOBAL_SEED)\n",
    "    return tf.add(mu, tf.multiply(epsilon, sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VRNN(object):\n",
    "    \"\"\"class for VRNN.\"\"\"\n",
    "    def __init__(self, input_shape, rnn_size, x_size, x_1_size, \n",
    "               z_size, z_1_size, phi_size, prior_size, theta_size, lr=.01):\n",
    "        self.batch_size = input_shape[0]\n",
    "        self.ntime_steps = input_shape[1]\n",
    "        self.nfeatures = input_shape[2]\n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.x_size = x_size\n",
    "        self.x_1_size = x_1_size\n",
    "        self.z_size = z_size\n",
    "        self.z_1_size = z_1_size\n",
    "        self.phi_size = phi_size\n",
    "        self.prior_size = prior_size\n",
    "        self.theta_size = theta_size\n",
    "        self.learning_rate=lr\n",
    "        \n",
    "        self.inputs = self._define_input_placeholder(input_shape)\n",
    "        self._build()\n",
    "\n",
    "    def _define_input_placeholder(self, input_shape):\n",
    "        return tf.placeholder(tf.float32, shape=input_shape, name=\"x_t\")\n",
    "\n",
    "    def _tensorboard_additions(self):\n",
    "        # get latest values from VRNN so that you can visualize how the last time-step changes over time\n",
    "\n",
    "        with tf.name_scope(\"prior_sample\"):\n",
    "            prior_mu, prior_sigma = self.priors;\n",
    "            prior_t = tf_sample(self.batch_size, self.z_size, prior_mu[:,-1], prior_sigma[:,-1])\n",
    "            tf.summary.histogram(\"prior_t\", prior_t) # should be a unit gaussian\n",
    "        \n",
    "        with tf.name_scope(\"z_sample\"):\n",
    "            z_t = self.calculated_posteriors;\n",
    "            z_t = z_t[:,-1]\n",
    "            tf.summary.histogram(\"z_t\", z_t)\n",
    "\n",
    "        with tf.name_scope(\"x_sample\"):\n",
    "            x_mu, x_sigma = self.likelihoods;\n",
    "            x_t = tf_sample(self.batch_size, self.x_size, x_mu[:,-1], x_sigma[:,-1])\n",
    "            tf.summary.histogram(\"x_t\", x_t)\n",
    "        \n",
    "        \n",
    "    def _build(self):\n",
    "        self._init_cell()\n",
    "        # transpose input so T X N X F instead of N X T X F\n",
    "        # this allows you to do one time-step at a time for all batches in tf.scan\n",
    "        self.inputs_t = tf.transpose(self.inputs, [1, 0, 2])\n",
    "        self.outputs = self._scan_sequence(self.inputs_t)   \n",
    "\n",
    "        self._tensorboard_additions()\n",
    "\n",
    "        with tf.name_scope(\"negative-log-likelihood\"):\n",
    "            self.nll = negative_log_likelihood_gaussian(self.inputs_t, \n",
    "                                                        self.outputs[-2], self.outputs[-1])\n",
    "            self.nll_r = flatten(self.nll)\n",
    "        with tf.name_scope(\"kl-Divergence\"):\n",
    "            self.klgg = kl_gaussian_gaussian(self.outputs[5], self.outputs[6], \n",
    "                                             self.outputs[8], self.outputs[9])\n",
    "            self.klgg_r = flatten(self.klgg)\n",
    "\n",
    "        with tf.name_scope(\"objective-function\"):\n",
    "            nll_mean=tf.reduce_mean(self.nll_r)\n",
    "            klgg_mean=tf.reduce_mean(self.klgg_r)\n",
    "            self.objective = tf.add(tf.reduce_mean(self.nll_r),tf.reduce_mean(self.klgg_r))\n",
    "            \n",
    "            tf.summary.scalar(\"negative_log-likelihood\", nll_mean)\n",
    "            tf.summary.scalar(\"KL_divergence\", klgg_mean)\n",
    "            tf.summary.scalar(\"ELBO\", self.objective)\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(self.objective)\n",
    "\n",
    "    def _init_cell(self):\n",
    "        with tf.variable_scope(\"lstm\"):\n",
    "            try:\n",
    "                self.cell = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True)\n",
    "            except:\n",
    "                self.cell = tf.contrib.rnn.LSTMCell(rnn_size, state_is_tuple=True, reuse=True)\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def _calculate_prior(self, hidden_state):\n",
    "        \"\"\"\n",
    "        calculate your prior based on your previous state information, i.e. z ~ p(z)\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"prior\"):\n",
    "            prior = fc_act(hidden_state, prior_size, act=tf.nn.relu, name=\"fc_prior\")\n",
    "        with tf.variable_scope(\"prior_mu\"):\n",
    "            prior_mu = fc_act(prior, z_size, name=\"fc_prior_mu\")\n",
    "        with tf.variable_scope(\"prior_sigma\"):\n",
    "            prior_sigma = fc_act(prior, z_size, act=tf.nn.softplus, name=\"fc_prior_sigma\")\n",
    "        return [prior, \n",
    "                prior_mu, \n",
    "                prior_sigma]\n",
    "        \n",
    "    def _encode_x(self, x_t, hidden_state):\n",
    "        \"\"\"\n",
    "        infer the latent values given z, i.e. z ~ p(z|x)\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"x_1\"):\n",
    "            x_1 = fc_act(x_t, self.x_1_size, act=tf.nn.relu, name=\"fc_x_1\")\n",
    "        with tf.variable_scope(\"phi\"):\n",
    "            phi = fc_act(tf.concat([x_1, hidden_state], axis=1), self.phi_size, act=tf.nn.relu, name=\"fc_phi\")\n",
    "        with tf.variable_scope(\"phi_mu\"):\n",
    "            phi_mu = fc_act(phi, self.z_size, name=\"fc_phi_mu\")\n",
    "        with tf.variable_scope(\"phi_sigma\"):\n",
    "            phi_sigma = fc_act(phi, self.z_size, act=tf.nn.softplus, name=\"fc_phi_sigma\")\n",
    "        epsilon = tf.random_normal(shape=[x_t.get_shape().as_list()[0], self.z_size], seed=GLOBAL_SEED)\n",
    "        # z = mu + epsilon*sigma\n",
    "        z_t = tf.add(phi_mu, tf.multiply(epsilon, phi_sigma))\n",
    "        return [x_1, \n",
    "                phi,\n",
    "                phi_mu,\n",
    "                phi_sigma,\n",
    "                z_t]\n",
    "    \n",
    "    def _decode_z(self, z_t, hidden_state):\n",
    "        \"\"\"\n",
    "        generate the mean and variance for your data from latent values z, i.e. x ~ q(x|z)\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"z_1\"):\n",
    "            z_1 = fc_act(z_t, self.z_1_size, act=tf.nn.relu, name=\"fc_z_1\")\n",
    "        with tf.variable_scope(\"theta\"):\n",
    "            theta = fc_act(tf.concat([z_1, hidden_state],axis=1), self.theta_size, act=tf.nn.relu, name=\"fc_theta\")\n",
    "        with tf.variable_scope(\"theta_mu\"):\n",
    "            theta_mu = fc_act(theta, self.x_size, name=\"fc_theta_mu\")\n",
    "        with tf.variable_scope(\"theta_sigma\"):\n",
    "            theta_sigma = fc_act(theta, self.x_size, act=tf.nn.softplus, name=\"fc_theta_sigma\")\n",
    "        \n",
    "        return [z_1,\n",
    "                theta,\n",
    "                theta_mu,\n",
    "                theta_sigma]\n",
    "    def _rnn_recurrence(self, x_1, z_1, cell_state, hidden_state):\n",
    "        \"\"\"\n",
    "        use x_1 and z_1 and the previous hidden state to update lstm cell\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return self.cell(tf.concat([x_1, z_1], axis=1), [cell_state, hidden_state], \"rnn\")\n",
    "        except:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "            return self.cell(tf.concat([x_1, z_1], axis=1), [cell_state, hidden_state], \"rnn\")\n",
    "\n",
    "        \n",
    "    def _vrnn_step(self, vals, x_t):\n",
    "        \"\"\"\n",
    "        A single step on the time-series (or batch) using the VRNN. \n",
    "        \"\"\"\n",
    "        # I chose to store them all but you don't HAVE to do this. \n",
    "        # This is a convenient thing to do however so you can check values later on\n",
    "\n",
    "        prev_hidden = vals[1]\n",
    "        # mu_z, sigma_z = f(x, ht-1)\n",
    "        encoding_vals = [x_1, \n",
    "                phi,\n",
    "                phi_mu,\n",
    "                phi_sigma,\n",
    "                z_t] = \\\n",
    "                    self._encode_x(x_t, prev_hidden)\n",
    "        \n",
    "        # mu_phi, sigma_phi = f(ht-1)\n",
    "        prior_vals = [prior, \n",
    "                prior_mu, \n",
    "                prior_sigma] = \\\n",
    "                    self._calculate_prior(prev_hidden)\n",
    "\n",
    "        # mu_x , sigma_x = f(z, ht-1)\n",
    "        decoding_vals = [z_1,\n",
    "                theta,\n",
    "                theta_mu,\n",
    "                theta_sigma] = \\\n",
    "                    self._decode_z(z_t, prev_hidden)\n",
    "            \n",
    "        output, state = self._rnn_recurrence(x_1, z_1, vals[0], vals[1])\n",
    "\n",
    "        return [\n",
    "            state[0], # vals[0],\n",
    "            state[1], # vals[1],\n",
    "            output, # vals[2],\n",
    "            x_1, # vals[3],\n",
    "            phi, # vals[4],\n",
    "            phi_mu, # vals[5],\n",
    "            phi_sigma, # vals[6],\n",
    "            prior, # vals[7],\n",
    "            prior_mu, # vals[8],\n",
    "            prior_sigma, # vals[9],\n",
    "            z_t, # vals[10],\n",
    "            z_1, # vals[11],\n",
    "            theta, # vals[12],\n",
    "            theta_mu, # vals[13],\n",
    "            theta_sigma # vals[14]\n",
    "        ]\n",
    "   \n",
    "    def _scan_sequence(self, sequence):\n",
    "        with tf.name_scope(\"scan_input\"):\n",
    "            return tf.scan(self._vrnn_step, sequence, initializer=[\n",
    "                    self.state[0], # cell_state - 0,\n",
    "                    self.state[1], # hidden_state - 1,\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.rnn_size), dtype=np.float32)), # cell_output - 2\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.x_size), dtype=np.float32)), # x_1 - 3\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.phi_size), dtype=np.float32)), # phi - 4\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # phi_mu - 5\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # phi_sigma - 6\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.prior_size), dtype=np.float32)), # prior - 7\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # prior_mu - 8\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # prior_sigma - 9\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # z_t - 10\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.z_size), dtype=np.float32)), # z_1 - 11\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.theta_size), dtype=np.float32)), # theta - 12\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.x_size), dtype=np.float32)), # theta_mu - 13\n",
    "                    tf.Variable(np.zeros((self.batch_size, self.x_size), dtype=np.float32)) # theta_sigma - 14\n",
    "            ])\n",
    "    \n",
    "    @property\n",
    "    def loss(self): return self.objective\n",
    "    \n",
    "    @property\n",
    "    def x(self): return self.inputs\n",
    "    \n",
    "    @property\n",
    "    def optimization_step(self): return self.train_step\n",
    "\n",
    "    @property\n",
    "    def cell_states(self): \n",
    "        return tf.transpose(self.outputs[0], [1, 0, 2])\n",
    "        \n",
    "    @property\n",
    "    def hidden_states(self): \n",
    "        return tf.transpose(self.outputs[1], [1, 0, 2])\n",
    "\n",
    "    @property\n",
    "    def priors(self): \n",
    "        \"\"\"\n",
    "        mean and variance for prior on z p(z)\n",
    "        transposed from T X N X F to be N X T X F\n",
    "        \"\"\"\n",
    "        return tf.transpose(self.outputs[8], [1, 0, 2]), tf.transpose(self.outputs[9], [1, 0, 2])\n",
    "\n",
    "    @property\n",
    "    def calculated_posteriors(self): \n",
    "        \"\"\"\n",
    "        mean and variance to infer z from q(z|x)\n",
    "        transposed from T X N X F to be N X T X F\n",
    "        \"\"\"\n",
    "        return tf.transpose(self.outputs[10], [1, 0, 2])\n",
    "    \n",
    "    @property\n",
    "    def posteriors(self): \n",
    "        \"\"\"\n",
    "        mean and variance to infer z from q(z|x)\n",
    "        transposed from T X N X F to be N X T X F\n",
    "        \"\"\"\n",
    "        return tf.transpose(self.outputs[5], [1, 0, 2]), tf.transpose(self.outputs[6], [1, 0, 2])\n",
    "\n",
    "    @property\n",
    "    def likelihoods(self): \n",
    "        \"\"\"\n",
    "        mean and variance to reconstruct x from p(x|z)\n",
    "        transposed from T X N X F to be N X T X F\n",
    "        \"\"\"\n",
    "        return tf.transpose(self.outputs[-2], [1, 0, 2]), tf.transpose(self.outputs[-1], [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test computing latent variable\n",
    "batch_size=10\n",
    "\n",
    "rnn_size=20\n",
    "x_size=x_data.shape[2]\n",
    "x_1_size=x_data.shape[2]\n",
    "\n",
    "z_size=15\n",
    "z_1_size=15\n",
    "\n",
    "phi_size = 25   # N ~ q(z|x)\n",
    "prior_size = 25 # N ~ p(z)\n",
    "theta_size = 25 # N ~ p(x|z)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "vrnn = VRNN([batch_size, n_points, n_features], rnn_size, x_size, x_1_size, \n",
    "       z_size, z_1_size, phi_size, prior_size, theta_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define session and model saver\n",
    "def get_new_session():\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    return sess\n",
    "sess = get_new_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "LOGDIR = \"/tmp/test1/\"\n",
    "if os.path.exists(LOGDIR):\n",
    "    shutil.rmtree(LOGDIR)\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "writer = tf.summary.FileWriter(LOGDIR)\n",
    "writer.add_graph(sess.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, data, model, summaries, writer, batch_size, run, tensor_freq=10):\n",
    "    i=0\n",
    "    nbatches=max(data.shape[0]//batch_size,1)\n",
    "    print(run, )\n",
    "    for j in range(nbatches):\n",
    "        batch = x_data[j*batch_size:(j+1)*batch_size]\n",
    "        if i % tensor_freq == 0:\n",
    "            s = sess.run(summaries, feed_dict={model.x: batch})\n",
    "            writer.add_summary(s, run*nbatches+i)\n",
    "        sess.run(model.optimization_step, feed_dict={model.x: batch})\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    train(sess, x_train, vrnn, merged_summary, writer, batch_size, run=i, tensor_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(2000,4000):\n",
    "    train(sess, x_train, vrnn, merged_summary, writer, batch_size, run=i, tensor_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstruct(sess, data, model, batch_size):\n",
    "    mus = []\n",
    "    sigmas = []\n",
    "    nbatches=max(data.shape[0]//batch_size,1)\n",
    "    for j in range(nbatches):\n",
    "        batch = x_data[j*batch_size:(j+1)*batch_size]\n",
    "        reconstructions = sess.run([model.likelihoods], feed_dict={model.x: batch})\n",
    "        mu, sigma = reconstructions[0]\n",
    "        mus.append(mu)\n",
    "        sigmas.append(sigma)\n",
    "    mus=mus[0]\n",
    "    sigmas=sigmas[0]\n",
    "    return mus, sigmas\n",
    "def plot_reconstruction(mus, sigmas, data, index):\n",
    "    mu_series=mus[index]\n",
    "    sigma_series=sigmas[index]\n",
    "    actual = x_test[index]\n",
    "\n",
    "    reconstruction = np.random.normal(mu_series, sigma_series)\n",
    "    print(\"reconstruction\", reconstruction.shape)\n",
    "    print(\"actual\", actual.shape)\n",
    "    plt.plot(actual[:, 0], label=\"real\");\n",
    "    plt.plot(reconstruction[:,0], label=\"reconstruction\");\n",
    "    plt.legend()\n",
    "    plt.show();\n",
    "    plt.plot(actual[:, 1], label=\"real\");\n",
    "    plt.plot(reconstruction[:,1], label=\"reconstruction\");\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mus, sigmas = reconstruct(sess, x_train, vrnn, batch_size)\n",
    "plot_reconstruction(mus, sigmas, x_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess2 = get_new_session()\n",
    "j=0\n",
    "batch = x_data[j*batch_size:(j+1)*batch_size]\n",
    "cell, hidden, priors = sess.run([vrnn.cell_states, vrnn.hidden_states, vrnn.priors], feed_dict={vrnn.x: batch})\n",
    "cell = cell[0]\n",
    "hidden = hidden[0]\n",
    "priors = priors[0]\n",
    "mu = priors[0]\n",
    "sigma= priors[1]\n",
    "print(mu.shape, mu)\n",
    "print(sigma.shape, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# loop over learning rates\n",
    "# relative is computational time\n",
    "# wall time is actual time\n",
    "\n",
    "# play with embedding. look at the inferred z for different sequences. write them somehow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
